{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/signups.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load core data (always available)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m signups \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/signups.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load subscriptions with proper CSV parsing (handles embedded JSON in metadata column)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_subscriptions_csv\u001b[39m(filepath):\n",
      "File \u001b[1;32mc:\\Users\\paul0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paul0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\paul0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paul0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\paul0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/signups.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load core data (always available)\n",
    "signups = pd.read_csv('data/signups.csv')\n",
    "\n",
    "# Load subscriptions with proper CSV parsing (handles embedded JSON in metadata column)\n",
    "def load_subscriptions_csv(filepath):\n",
    "    \"\"\"Load subscriptions.csv handling embedded JSON in metadata column\"\"\"\n",
    "    rows = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "        for row in reader:\n",
    "            if len(row) >= 5:  # At least have key columns\n",
    "                if len(row) < len(headers):\n",
    "                    row = row + [''] * (len(headers) - len(row))\n",
    "                elif len(row) > len(headers):\n",
    "                    row = row[:len(headers)]\n",
    "                rows.append(row)\n",
    "    return pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "subscriptions = load_subscriptions_csv('data/subscriptions.csv')\n",
    "\n",
    "# Parse dates\n",
    "signups['created_at'] = pd.to_datetime(signups['created_at'])\n",
    "subscriptions['created_at'] = pd.to_datetime(subscriptions['created_at'], errors='coerce')\n",
    "\n",
    "# Convert company_id to numeric for proper matching\n",
    "subscriptions['company_id'] = pd.to_numeric(subscriptions['company_id'], errors='coerce')\n",
    "\n",
    "print(f\"=== CORE DATA ===\")\n",
    "print(f\"Signups: {len(signups)} companies\")\n",
    "print(f\"Subscriptions: {len(subscriptions)} records\")\n",
    "print(f\"Signups date range: {signups['created_at'].min()} to {signups['created_at'].max()}\")\n",
    "\n",
    "# Load additional data if available\n",
    "def load_if_exists(filename, date_cols=None):\n",
    "    path = f'data/{filename}'\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, on_bad_lines='skip')\n",
    "        if date_cols:\n",
    "            for col in date_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"  {filename}: {len(df)} records\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"  {filename}: NOT FOUND (run query to export)\")\n",
    "        return None\n",
    "\n",
    "print(f\"\\n=== ADDITIONAL DATA ===\")\n",
    "bots = load_if_exists('bots.csv', ['created_at', 'updated_at'])\n",
    "credit_wallet = load_if_exists('credit_wallet.csv', ['created_at', 'current_period_start', 'current_period_end'])\n",
    "wallet_transactions = load_if_exists('wallet_transactions.csv', ['created_at'])\n",
    "stripe_invoices = load_if_exists('stripe_invoices.csv', ['created_at', 'paid_at', 'period_start', 'period_end'])\n",
    "\n",
    "template_usage = load_if_exists('template_usage_connect.csv', ['first_event', 'last_event'])\n",
    "\n",
    "sessions_duration = load_if_exists('sessions_duration.csv')\n",
    "if sessions_duration is not None:\n",
    "    sessions_duration.columns = ['company_id', 'total_time_minutes', 'avg_session_minutes', 'session_count']\n",
    "    sessions_duration['company_id'] = pd.to_numeric(sessions_duration['company_id'], errors='coerce')\n",
    "\n",
    "nodes_usage = load_if_exists('nodes_usage.csv')\n",
    "if nodes_usage is not None:\n",
    "    nodes_usage['company_id'] = pd.to_numeric(nodes_usage['company_id'], errors='coerce')\n",
    "\n",
    "nodes_usage = load_if_exists('nodes_used.csv')\n",
    "if nodes_usage is not None:\n",
    "    nodes_usage['company_id'] = pd.to_numeric(nodes_usage['company_id'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Signups Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan breakdown\n",
    "print(\"=== SIGNUPS BY PLAN ===\")\n",
    "plan_counts = signups['plan'].value_counts()\n",
    "plan_pct = (plan_counts / len(signups) * 100).round(1)\n",
    "display(pd.DataFrame({'Count': plan_counts, 'Percentage': plan_pct}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Production breakdown\n",
    "print(\"=== ENVIRONMENT ===\")\n",
    "display(signups['environment'].value_counts())\n",
    "\n",
    "print(\"\\n=== IN PRODUCTION (flag) ===\")\n",
    "display(signups['in_production'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily signups trend\n",
    "signups['date'] = signups['created_at'].dt.date\n",
    "daily_signups = signups.groupby('date').size().reset_index(name='count')\n",
    "print(f\"Average signups per day: {daily_signups['count'].mean():.1f}\")\n",
    "print(f\"Min: {daily_signups['count'].min()}, Max: {daily_signups['count'].max()}\")\n",
    "daily_signups.tail(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subscriptions Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscription status breakdown\n",
    "print(\"=== SUBSCRIPTION STATUS ===\")\n",
    "display(subscriptions['status'].value_counts())\n",
    "\n",
    "print(\"\\n=== SUBSCRIPTION BY PRODUCT ===\")\n",
    "product_counts = subscriptions['product_name'].value_counts()\n",
    "display(product_counts)\n",
    "\n",
    "# Show the actual product names we're detecting\n",
    "print(\"\\n=== PRODUCT DETECTION ===\")\n",
    "brain_pattern = subscriptions['product_name'].str.contains('Brain', case=False, na=False)\n",
    "connect_pattern = subscriptions['product_name'].str.contains('Connect', case=False, na=False)\n",
    "print(f\"Brain-related products: {brain_pattern.sum()} subscriptions\")\n",
    "print(f\"Connect-related products: {connect_pattern.sum()} subscriptions\")\n",
    "\n",
    "print(\"\\n=== PRODUCT x STATUS ===\")\n",
    "display(pd.crosstab(subscriptions['product_name'], subscriptions['status'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Joining Signups with Subscriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many signups have subscriptions?\n",
    "companies_with_subs = subscriptions['company_id'].unique()\n",
    "signups['has_subscription'] = signups['company_id'].isin(companies_with_subs)\n",
    "\n",
    "print(\"=== SIGNUPS WITH SUBSCRIPTIONS ===\")\n",
    "sub_counts = signups['has_subscription'].value_counts()\n",
    "sub_pct = (sub_counts / len(signups) * 100).round(1)\n",
    "display(pd.DataFrame({'Count': sub_counts, 'Percentage': sub_pct}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subscription summary per company\n",
    "sub_summary = subscriptions.groupby('company_id').agg({\n",
    "    'subscription_id': 'count',\n",
    "    'status': lambda x: list(x.unique()),\n",
    "    'product_name': lambda x: list(x.unique()),\n",
    "    'created_at': 'min'\n",
    "}).reset_index()\n",
    "sub_summary.columns = ['company_id', 'subscription_count', 'statuses', 'products', 'first_subscription']\n",
    "\n",
    "# Check for active subscriptions\n",
    "sub_summary['has_active'] = sub_summary['statuses'].apply(lambda x: 'ACTIVE' in x)\n",
    "sub_summary['has_trialing'] = sub_summary['statuses'].apply(lambda x: 'TRIALING' in x)\n",
    "# Check for Brain and Connect products using pattern matching\n",
    "# This handles: \"Brain studio\", \"Brain conversaciones\", \"Connect\", \"Plan Connect\"\n",
    "def has_brain_product(products):\n",
    "    if not isinstance(products, list):\n",
    "        return False\n",
    "    return any('brain' in str(p).lower() for p in products)\n",
    "\n",
    "def has_connect_product(products):\n",
    "    if not isinstance(products, list):\n",
    "        return False\n",
    "    return any('connect' in str(p).lower() for p in products)\n",
    "\n",
    "sub_summary['has_brain_studio'] = sub_summary['products'].apply(has_brain_product)\n",
    "sub_summary['has_connect'] = sub_summary['products'].apply(has_connect_product)\n",
    "\n",
    "# Track Connect-specific statuses for funnel\n",
    "connect_subs = subscriptions[subscriptions['product_name'].str.contains('Connect', case=False, na=False)]\n",
    "connect_active_df = connect_subs[connect_subs['status'] == 'ACTIVE'].groupby('company_id').size().reset_index(name='connect_active_count')\n",
    "connect_trialing_df = connect_subs[connect_subs['status'] == 'TRIALING'].groupby('company_id').size().reset_index(name='connect_trialing_count')\n",
    "\n",
    "sub_summary = sub_summary.merge(connect_active_df, on='company_id', how='left')\n",
    "sub_summary = sub_summary.merge(connect_trialing_df, on='company_id', how='left')\n",
    "sub_summary['connect_active'] = sub_summary['connect_active_count'].fillna(0) > 0\n",
    "sub_summary['connect_trialing'] = sub_summary['connect_trialing_count'].fillna(0) > 0\n",
    "\n",
    "# Merge with signups\n",
    "analysis = signups.merge(sub_summary, on='company_id', how='left')\n",
    "analysis['subscription_count'] = analysis['subscription_count'].fillna(0).astype(int)\n",
    "analysis['has_active'] = analysis['has_active'].fillna(False)\n",
    "analysis['has_trialing'] = analysis['has_trialing'].fillna(False)\n",
    "analysis['has_brain_studio'] = analysis['has_brain_studio'].fillna(False)\n",
    "analysis['has_connect'] = analysis['has_connect'].fillna(False)\n",
    "analysis['connect_active'] = analysis['connect_active'].fillna(False)\n",
    "analysis['connect_trialing'] = analysis['connect_trialing'].fillna(False)\n",
    "\n",
    "print(f\"Analysis dataset: {len(analysis)} companies\")\n",
    "print(f\"\\n=== PRODUCT DETECTION RESULTS ===\")\n",
    "print(f\"Companies with Brain Studio: {analysis['has_brain_studio'].sum()}\")\n",
    "print(f\"Companies with Connect: {analysis['has_connect'].sum()}\")\n",
    "print(f\"  - Connect Active (paid): {analysis['connect_active'].sum()}\")\n",
    "print(f\"  - Connect Trialing: {analysis['connect_trialing'].sum()}\")\n",
    "analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conversion Funnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build conversion funnel\n",
    "total = len(analysis)\n",
    "has_any_sub = analysis['has_subscription'].sum()\n",
    "has_brain = analysis['has_brain_studio'].sum()\n",
    "has_connect = analysis['has_connect'].sum()\n",
    "connect_trialing = analysis['connect_trialing'].sum()\n",
    "connect_active = analysis['connect_active'].sum()\n",
    "has_active = analysis['has_active'].sum()\n",
    "in_production = analysis['in_production'].sum()\n",
    "\n",
    "print(\"=== OVERALL FUNNEL ===\")\n",
    "funnel = pd.DataFrame({\n",
    "    'Stage': ['Total Signups', 'Has Any Subscription', 'Has Brain Studio', 'Has Connect', 'Active Subscription', 'In Production (flag)'],\n",
    "    'Count': [total, has_any_sub, has_brain, has_connect, has_active, in_production],\n",
    "})\n",
    "funnel['Percentage'] = (funnel['Count'] / total * 100).round(1)\n",
    "funnel['Conversion'] = funnel['Percentage'].astype(str) + '%'\n",
    "display(funnel)\n",
    "\n",
    "print(\"\\n=== CONNECT-SPECIFIC FUNNEL ===\")\n",
    "connect_funnel = pd.DataFrame({\n",
    "    'Stage': ['Total Signups', 'Started Connect (trial or active)', 'Connect Trialing', 'Connect Active (Paid)'],\n",
    "    'Count': [total, has_connect, connect_trialing, connect_active],\n",
    "})\n",
    "connect_funnel['Percentage'] = (connect_funnel['Count'] / total * 100).round(1)\n",
    "connect_funnel['Conversion'] = connect_funnel['Percentage'].astype(str) + '%'\n",
    "\n",
    "# Trial to paid conversion\n",
    "if has_connect > 0:\n",
    "    trial_to_paid = connect_active / has_connect * 100\n",
    "    print(f\"Trial â†’ Paid Conversion: {trial_to_paid:.1f}%\")\n",
    "display(connect_funnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown by plan type\n",
    "print(\"=== CONVERSION BY PLAN TYPE ===\")\n",
    "plan_analysis = analysis.groupby('plan').agg({\n",
    "    'company_id': 'count',\n",
    "    'has_subscription': 'sum',\n",
    "    'has_active': 'sum',\n",
    "    'in_production': 'sum'\n",
    "}).reset_index()\n",
    "plan_analysis.columns = ['plan', 'total', 'has_subscription', 'has_active', 'in_production']\n",
    "plan_analysis['sub_rate'] = (plan_analysis['has_subscription'] / plan_analysis['total'] * 100).round(1)\n",
    "plan_analysis['active_rate'] = (plan_analysis['has_active'] / plan_analysis['total'] * 100).round(1)\n",
    "plan_analysis['prod_rate'] = (plan_analysis['in_production'] / plan_analysis['total'] * 100).round(1)\n",
    "plan_analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Service Deep Dive\n",
    "\n",
    "Focus on SELF_SERVICE users since those are the ones expected to convert on their own\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to self-service only\n",
    "self_service = analysis[analysis['plan'] == 'SELF_SERVICE'].copy()\n",
    "print(f\"Self-service signups: {len(self_service)}\")\n",
    "\n",
    "# Breakdown\n",
    "ss_total = len(self_service)\n",
    "ss_with_sub = self_service['has_subscription'].sum()\n",
    "ss_active = self_service['has_active'].sum()\n",
    "ss_trialing = self_service['has_trialing'].sum()\n",
    "ss_in_prod = self_service['in_production'].sum()\n",
    "\n",
    "print(f\"\\n=== SELF-SERVICE FUNNEL ===\")\n",
    "print(f\"Total: {ss_total}\")\n",
    "print(f\"Has subscription: {ss_with_sub} ({ss_with_sub/ss_total*100:.1f}%)\")\n",
    "print(f\"  - Active: {ss_active} ({ss_active/ss_total*100:.1f}%)\")\n",
    "print(f\"  - Trialing: {ss_trialing} ({ss_trialing/ss_total*100:.1f}%)\")\n",
    "print(f\"In Production: {ss_in_prod} ({ss_in_prod/ss_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-service WITHOUT subscription - who are they?\n",
    "ss_no_sub = self_service[~self_service['has_subscription']]\n",
    "print(f\"Self-service WITHOUT subscription: {len(ss_no_sub)}\")\n",
    "print(f\"\\nEnvironment breakdown:\")\n",
    "display(ss_no_sub['environment'].value_counts())\n",
    "print(f\"\\nIn production:\")\n",
    "display(ss_no_sub['in_production'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Insights Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. TOTAL SIGNUPS: {len(signups)}\")\n",
    "print(f\"   - Self-Service: {len(signups[signups['plan']=='SELF_SERVICE'])} ({len(signups[signups['plan']=='SELF_SERVICE'])/len(signups)*100:.1f}%)\")\n",
    "print(f\"   - Enterprise: {len(signups[signups['plan']=='ENTERPRISE'])} ({len(signups[signups['plan']=='ENTERPRISE'])/len(signups)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. SUBSCRIPTION STATUS:\")\n",
    "print(f\"   - Companies with ANY subscription: {has_any_sub} ({has_any_sub/total*100:.1f}%)\")\n",
    "print(f\"   - Companies with ACTIVE subscription: {has_active} ({has_active/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. PRODUCTION STATUS:\")\n",
    "print(f\"   - In Production (flag=1): {in_production} ({in_production/total*100:.1f}%)\")\n",
    "print(f\"   - Environment=PRODUCTION: {len(signups[signups['environment']=='PRODUCTION'])} ({len(signups[signups['environment']=='PRODUCTION'])/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n4. POTENTIAL ISSUES:\")\n",
    "no_sub = total - has_any_sub\n",
    "print(f\"   - Signups WITHOUT any subscription: {no_sub} ({no_sub/total*100:.1f}%)\")\n",
    "print(f\"   - These users signed up but never started a trial/subscription\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CORRECTED Conversion Funnel (Using New Data Sources)\n",
    "\n",
    "The previous funnel was WRONG because Brain Studio subscriptions are auto-created and FREE.\n",
    "Now we use the correct data sources:\n",
    "- `bots.csv` - Shows actual bot creation and production channel connection\n",
    "- `credit_wallet.csv` - Shows conversation usage and free tier exhaustion\n",
    "- `stripe_invoices.csv` - Shows actual payments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CORRECTED funnel using new data sources\n",
    "# Focus on SELF_SERVICE signups (organic users expected to self-convert)\n",
    "\n",
    "self_service = signups[signups['plan'] == 'SELF_SERVICE'].copy()\n",
    "ss_total = len(self_service)\n",
    "print(f\"=== CORRECTED FUNNEL (SELF_SERVICE only) ===\")\n",
    "print(f\"Total Self-Service Signups: {ss_total}\")\n",
    "\n",
    "# Stage 1: Created Bot (from bots.csv)\n",
    "if bots is not None:\n",
    "    companies_with_bots = bots['company_id'].unique()\n",
    "    self_service['has_bot'] = self_service['company_id'].isin(companies_with_bots)\n",
    "    ss_with_bot = self_service['has_bot'].sum()\n",
    "    print(f\"\\n1. Created Bot: {ss_with_bot} ({ss_with_bot/ss_total*100:.1f}%)\")\n",
    "    \n",
    "    # Stage 2: Connected Production Channel (bots.in_production = 1)\n",
    "    prod_bots = bots[bots['in_production'] == 1]\n",
    "    companies_with_prod = prod_bots['company_id'].unique()\n",
    "    self_service['has_prod_channel'] = self_service['company_id'].isin(companies_with_prod)\n",
    "    ss_with_prod = self_service['has_prod_channel'].sum()\n",
    "    print(f\"2. Production Channel: {ss_with_prod} ({ss_with_prod/ss_total*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n[!] bots.csv not available - run queries/bots.sql\")\n",
    "    self_service['has_bot'] = False\n",
    "    self_service['has_prod_channel'] = False\n",
    "\n",
    "# Stage 3: Used Conversations (from credit_wallet)\n",
    "if credit_wallet is not None:\n",
    "    # Companies that have used any conversations\n",
    "    used_convos = credit_wallet[credit_wallet['total_used'] > 0]\n",
    "    companies_used = used_convos['company_id'].unique()\n",
    "    self_service['used_conversations'] = self_service['company_id'].isin(companies_used)\n",
    "    ss_used = self_service['used_conversations'].sum()\n",
    "    print(f\"3. Used Conversations: {ss_used} ({ss_used/ss_total*100:.1f}%)\")\n",
    "    \n",
    "    # Stage 4: Exceeded Free Tier\n",
    "    exceeded = credit_wallet[credit_wallet['exceeded_free_tier'] == 1]\n",
    "    companies_exceeded = exceeded['company_id'].unique()\n",
    "    self_service['exceeded_free_tier'] = self_service['company_id'].isin(companies_exceeded)\n",
    "    ss_exceeded = self_service['exceeded_free_tier'].sum()\n",
    "    print(f\"4. Exceeded Free Tier: {ss_exceeded} ({ss_exceeded/ss_total*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n[!] credit_wallet.csv not available - run queries/credit_wallet.sql\")\n",
    "    self_service['used_conversations'] = False\n",
    "    self_service['exceeded_free_tier'] = False\n",
    "\n",
    "# Stage 5: Actually Paid (from stripe_invoices)\n",
    "if stripe_invoices is not None:\n",
    "    paid_invoices = stripe_invoices[stripe_invoices['amount_paid'] > 0]\n",
    "    companies_paid = paid_invoices['company_id'].unique()\n",
    "    self_service['actually_paid'] = self_service['company_id'].isin(companies_paid)\n",
    "    ss_paid = self_service['actually_paid'].sum()\n",
    "    print(f\"5. Actually Paid: {ss_paid} ({ss_paid/ss_total*100:.1f}%)\")\n",
    "    \n",
    "    # This is the REAL conversion rate!\n",
    "    print(f\"\\n>>> REAL CONVERSION RATE: {ss_paid/ss_total*100:.2f}% <<<\")\n",
    "else:\n",
    "    print(\"\\n[!] stripe_invoices.csv not available - run queries/stripe_invoices.sql\")\n",
    "    self_service['actually_paid'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corrected funnel\n",
    "funnel_stages = ['Signup', 'Created Bot', 'Production Channel', 'Used Conversations', 'Exceeded Free', 'Actually Paid']\n",
    "funnel_values = [ss_total]\n",
    "\n",
    "if bots is not None:\n",
    "    funnel_values.append(ss_with_bot)\n",
    "    funnel_values.append(ss_with_prod)\n",
    "else:\n",
    "    funnel_values.extend([0, 0])\n",
    "\n",
    "if credit_wallet is not None:\n",
    "    funnel_values.append(ss_used)\n",
    "    funnel_values.append(ss_exceeded)\n",
    "else:\n",
    "    funnel_values.extend([0, 0])\n",
    "\n",
    "if stripe_invoices is not None:\n",
    "    funnel_values.append(ss_paid)\n",
    "else:\n",
    "    funnel_values.append(0)\n",
    "\n",
    "corrected_funnel = pd.DataFrame({\n",
    "    'Stage': funnel_stages,\n",
    "    'Count': funnel_values,\n",
    "})\n",
    "corrected_funnel['Percentage'] = (corrected_funnel['Count'] / ss_total * 100).round(1)\n",
    "corrected_funnel['Drop-off'] = corrected_funnel['Count'].diff().fillna(0).astype(int)\n",
    "corrected_funnel['Drop-off %'] = (corrected_funnel['Drop-off'].abs() / corrected_funnel['Count'].shift(1) * 100).fillna(0).round(1)\n",
    "\n",
    "print(\"=== CORRECTED FUNNEL TABLE ===\")\n",
    "corrected_funnel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Dive: Bots Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bots is not None:\n",
    "    print(\"=== BOTS OVERVIEW ===\")\n",
    "    print(f\"Total bots: {len(bots)}\")\n",
    "    print(f\"Unique companies with bots: {bots['company_id'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n=== BOT TYPES ===\")\n",
    "    display(bots['type'].value_counts())\n",
    "    \n",
    "    print(\"\\n=== BOT STATES ===\")\n",
    "    display(bots['state'].value_counts())\n",
    "    \n",
    "    print(\"\\n=== PRODUCTION STATUS ===\")\n",
    "    display(bots['in_production'].value_counts())\n",
    "    \n",
    "    print(\"\\n=== CONNECTED STATUS ===\")\n",
    "    if 'connected' in bots.columns:\n",
    "        display(bots['connected'].value_counts())\n",
    "    \n",
    "    # Bots per company\n",
    "    bots_per_company = bots.groupby('company_id').size().describe()\n",
    "    print(\"\\n=== BOTS PER COMPANY ===\")\n",
    "    display(bots_per_company)\n",
    "else:\n",
    "    print(\"bots.csv not available - run queries/bots.sql and export\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deep Dive: Credit Wallet & Free Tier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if credit_wallet is not None:\n",
    "    print(\"=== CREDIT WALLET OVERVIEW ===\")\n",
    "    print(f\"Total wallets: {len(credit_wallet)}\")\n",
    "    print(f\"Unique companies: {credit_wallet['company_id'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n=== FREE CONVERSATIONS ALLOCATION ===\")\n",
    "    display(credit_wallet['free_conversations'].describe())\n",
    "    \n",
    "    print(\"\\n=== TOTAL USED ===\")\n",
    "    display(credit_wallet['total_used'].describe())\n",
    "    \n",
    "    print(\"\\n=== EXCEEDED FREE TIER ===\")\n",
    "    display(credit_wallet['exceeded_free_tier'].value_counts())\n",
    "    \n",
    "    # How many used vs how many exceeded\n",
    "    used_any = (credit_wallet['total_used'] > 0).sum()\n",
    "    exceeded = (credit_wallet['exceeded_free_tier'] == 1).sum()\n",
    "    print(f\"\\nCompanies that used ANY conversations: {used_any}\")\n",
    "    print(f\"Companies that EXCEEDED free tier: {exceeded}\")\n",
    "    print(f\"Conversion from usage to paid tier: {exceeded/used_any*100:.1f}%\" if used_any > 0 else \"N/A\")\n",
    "    \n",
    "    # Distribution of usage as % of free tier\n",
    "    if 'free_tier_usage_pct' in credit_wallet.columns:\n",
    "        print(\"\\n=== FREE TIER USAGE % DISTRIBUTION ===\")\n",
    "        usage_buckets = pd.cut(credit_wallet['free_tier_usage_pct'], \n",
    "                               bins=[0, 25, 50, 75, 100, 150, 200, float('inf')],\n",
    "                               labels=['0-25%', '25-50%', '50-75%', '75-100%', '100-150%', '150-200%', '>200%'])\n",
    "        display(usage_buckets.value_counts().sort_index())\n",
    "else:\n",
    "    print(\"credit_wallet.csv not available - run queries/credit_wallet.sql and export\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deep Dive: Payments & Revenue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Combined Analysis for Streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive analysis dataset with all new data sources\n",
    "# Start with signups base\n",
    "final_analysis = signups.copy()\n",
    "\n",
    "# Add bot info\n",
    "if bots is not None:\n",
    "    bot_summary = bots.groupby('company_id').agg({\n",
    "        'bot_id': 'count',\n",
    "        'in_production': 'sum',\n",
    "        'type': lambda x: list(x.unique()),\n",
    "        'created_at': 'min'\n",
    "    }).reset_index()\n",
    "    bot_summary.columns = ['company_id', 'bot_count', 'production_bots', 'bot_types', 'first_bot_created']\n",
    "    final_analysis = final_analysis.merge(bot_summary, on='company_id', how='left')\n",
    "    final_analysis['bot_count'] = final_analysis['bot_count'].fillna(0).astype(int)\n",
    "    final_analysis['production_bots'] = final_analysis['production_bots'].fillna(0).astype(int)\n",
    "    final_analysis['has_bot'] = final_analysis['bot_count'] > 0\n",
    "    final_analysis['has_prod_channel'] = final_analysis['production_bots'] > 0\n",
    "\n",
    "# Add credit wallet info\n",
    "if credit_wallet is not None:\n",
    "    wallet_cols = ['company_id', 'total_used', 'free_conversations', 'exceeded_free_tier', 'balance']\n",
    "    wallet_cols = [c for c in wallet_cols if c in credit_wallet.columns]\n",
    "    final_analysis = final_analysis.merge(credit_wallet[wallet_cols], on='company_id', how='left')\n",
    "    final_analysis['total_used'] = final_analysis['total_used'].fillna(0)\n",
    "    final_analysis['exceeded_free_tier'] = final_analysis['exceeded_free_tier'].fillna(0).astype(int)\n",
    "    final_analysis['used_conversations'] = final_analysis['total_used'] > 0\n",
    "\n",
    "# Add payment info from stripe invoices\n",
    "if stripe_invoices is not None:\n",
    "    payment_summary = stripe_invoices.groupby('company_id').agg({\n",
    "        'amount_paid': 'sum',\n",
    "        'paid_at': 'min'\n",
    "    }).reset_index()\n",
    "    payment_summary.columns = ['company_id', 'total_paid', 'first_payment']\n",
    "    final_analysis = final_analysis.merge(payment_summary, on='company_id', how='left')\n",
    "    final_analysis['total_paid'] = final_analysis['total_paid'].fillna(0)\n",
    "    final_analysis['actually_paid'] = final_analysis['total_paid'] > 0\n",
    "\n",
    "# Save for Streamlit\n",
    "final_analysis.to_csv('data/analysis_combined.csv', index=False)\n",
    "print(f\"Saved: data/analysis_combined.csv ({len(final_analysis)} companies)\")\n",
    "print(f\"\\nColumns: {list(final_analysis.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined analysis for Streamlit\n",
    "print(\"=== FINAL ANALYSIS SUMMARY ===\")\n",
    "print(f\"Total companies: {len(analysis)}\")\n",
    "print(f\"\\nProduct subscriptions:\")\n",
    "print(f\"  - Brain Studio: {analysis['has_brain_studio'].sum()}\")\n",
    "print(f\"  - Connect: {analysis['has_connect'].sum()}\")\n",
    "print(f\"    - Trialing: {analysis['connect_trialing'].sum()}\")\n",
    "print(f\"    - Active (Paid): {analysis['connect_active'].sum()}\")\n",
    "\n",
    "analysis.to_csv('data/analysis_combined.csv', index=False)\n",
    "print(\"\\nSaved: data/analysis_combined.csv\")\n",
    "print(f\"Columns: {list(analysis.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. User Sessions & Retention Analysis\n",
    "\n",
    "The retention curve needs to be calculated correctly:\n",
    "- **Current issue**: Using \"last_session >= X days after signup\" is WRONG\n",
    "- **Correct approach**: Count companies that logged in during each week period after signup\n",
    "\n",
    "We need to join `user_sessions.csv` with `signups.csv` to calculate:\n",
    "1. Days from signup to first session\n",
    "2. Days from signup to last session  \n",
    "3. Whether they were active during Week 1, Week 2, Week 3, Week 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user sessions data\n",
    "user_sessions_path = 'data/user_sessions.csv'\n",
    "if os.path.exists(user_sessions_path):\n",
    "    user_sessions = pd.read_csv(user_sessions_path)\n",
    "    print(f\"User sessions: {len(user_sessions)} companies\")\n",
    "    print(f\"Columns: {list(user_sessions.columns)}\")\n",
    "    \n",
    "    # Parse dates - handle ISO format with Z suffix\n",
    "    user_sessions['first_session'] = pd.to_datetime(user_sessions['first_session'], utc=True).dt.tz_convert(None)\n",
    "    user_sessions['last_session'] = pd.to_datetime(user_sessions['last_session'], utc=True).dt.tz_convert(None)\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"\\n=== SESSION STATS ===\")\n",
    "    print(f\"Days active distribution:\")\n",
    "    display(user_sessions['days_active'].describe())\n",
    "    \n",
    "    # How many have 7+ days active?\n",
    "    print(f\"\\n=== ACTIVITY THRESHOLDS ===\")\n",
    "    print(f\"Companies with 1+ days active: {(user_sessions['days_active'] >= 1).sum()}\")\n",
    "    print(f\"Companies with 7+ days active: {(user_sessions['days_active'] >= 7).sum()}\")\n",
    "    print(f\"Companies with 14+ days active: {(user_sessions['days_active'] >= 14).sum()}\")\n",
    "    print(f\"Companies with 28+ days active: {(user_sessions['days_active'] >= 28).sum()}\")\n",
    "else:\n",
    "    print(\"user_sessions.csv not found - run MongoDB query to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join user_sessions with signups to calculate retention properly\n",
    "# Filter to SELF_SERVICE and Nov 15+ signups\n",
    "\n",
    "# Get self-service signups from Nov 15, 2025 onwards\n",
    "nov15 = pd.Timestamp('2025-11-15')\n",
    "self_service_signups = signups[\n",
    "    (signups['plan'] == 'SELF_SERVICE') & \n",
    "    (signups['created_at'] >= nov15)\n",
    "].copy()\n",
    "\n",
    "print(f\"Self-service signups from Nov 15+: {len(self_service_signups)}\")\n",
    "\n",
    "# Merge with user_sessions\n",
    "user_sessions['company_id'] = pd.to_numeric(user_sessions['company_id'], errors='coerce')\n",
    "self_service_signups['company_id'] = pd.to_numeric(self_service_signups['company_id'], errors='coerce')\n",
    "\n",
    "retention_df = self_service_signups.merge(\n",
    "    user_sessions[['company_id', 'first_session', 'last_session', 'days_active', 'total_sessions']],\n",
    "    on='company_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Companies with session data: {retention_df['first_session'].notna().sum()}\")\n",
    "print(f\"Companies WITHOUT session data: {retention_df['first_session'].isna().sum()}\")\n",
    "\n",
    "# Show sample\n",
    "retention_df[['company_id', 'company_name', 'created_at', 'first_session', 'last_session', 'days_active']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CORRECT retention metrics\n",
    "# \n",
    "# Retention = \"Of users who signed up X days ago, how many logged in at least once after day Y?\"\n",
    "#\n",
    "# Key insight: We're measuring \"last_session - signup_date\" to see how long they stayed active\n",
    "# But this is STILL imperfect - ideally we'd have daily login data per company\n",
    "\n",
    "# Calculate days from signup to last session\n",
    "retention_df['signup_date'] = retention_df['created_at'].dt.normalize()\n",
    "retention_df['last_active_date'] = retention_df['last_session'].dt.normalize()\n",
    "retention_df['first_active_date'] = retention_df['first_session'].dt.normalize()\n",
    "\n",
    "# Days from signup to last activity\n",
    "retention_df['days_to_last_activity'] = (retention_df['last_active_date'] - retention_df['signup_date']).dt.days\n",
    "\n",
    "# Days from signup to first activity (how fast did they start?)\n",
    "retention_df['days_to_first_activity'] = (retention_df['first_active_date'] - retention_df['signup_date']).dt.days\n",
    "\n",
    "# How old is this signup?\n",
    "today = pd.Timestamp.now().normalize()\n",
    "retention_df['days_since_signup'] = (today - retention_df['signup_date']).dt.days\n",
    "\n",
    "print(\"=== RETENTION METRICS ===\")\n",
    "print(f\"Total signups in cohort: {len(retention_df)}\")\n",
    "print(f\"With any session data: {retention_df['days_to_last_activity'].notna().sum()}\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\n=== Days to Last Activity (distribution) ===\")\n",
    "display(retention_df['days_to_last_activity'].describe())\n",
    "\n",
    "print(\"\\n=== Days to First Activity (how fast they started) ===\")\n",
    "display(retention_df['days_to_first_activity'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT Retention Calculation\n",
    "#\n",
    "# For each period, we need to count:\n",
    "# 1. ELIGIBLE: signups that are OLD ENOUGH to be measured (signed up at least X days ago)\n",
    "# 2. RETAINED: of those eligible, how many had their last activity >= X days after signup\n",
    "#\n",
    "# This tells us: \"Of people who had time to reach week N, how many were still active at week N?\"\n",
    "\n",
    "def calculate_retention(df, period_days, min_signup_age):\n",
    "    \"\"\"\n",
    "    Calculate retention for a specific period.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'days_since_signup' and 'days_to_last_activity'\n",
    "        period_days: The retention period we're measuring (e.g., 7 for week 1)\n",
    "        min_signup_age: Minimum days since signup to be eligible\n",
    "    \n",
    "    Returns:\n",
    "        dict with eligible count, retained count, and rate\n",
    "    \"\"\"\n",
    "    # Filter to signups that are old enough\n",
    "    eligible = df[df['days_since_signup'] >= min_signup_age]\n",
    "    \n",
    "    # Of those, count how many had activity at or after the period\n",
    "    # (meaning they were still using the product at that point)\n",
    "    retained = eligible[eligible['days_to_last_activity'] >= period_days]\n",
    "    \n",
    "    eligible_count = len(eligible)\n",
    "    retained_count = len(retained[retained['days_to_last_activity'].notna()])\n",
    "    \n",
    "    rate = retained_count / eligible_count * 100 if eligible_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'eligible': eligible_count,\n",
    "        'retained': retained_count,\n",
    "        'rate': rate\n",
    "    }\n",
    "\n",
    "# Calculate retention for each period\n",
    "periods = [\n",
    "    ('Day 1', 1, 7),      # Logged in at least 1 day after signup, need 7 days of age\n",
    "    ('Week 1', 7, 14),    # Still active after 7 days, need 14 days of age\n",
    "    ('Week 2', 14, 21),   # Still active after 14 days, need 21 days of age\n",
    "    ('Week 3', 21, 28),   # Still active after 21 days, need 28 days of age\n",
    "    ('Week 4', 28, 35),   # Still active after 28 days, need 35 days of age\n",
    "    ('Week 8', 56, 63),   # Still active after 56 days, need 63 days of age\n",
    "]\n",
    "\n",
    "print(\"=== CORRECT RETENTION CALCULATION ===\")\n",
    "print(\"(Based on: last_activity >= period_days after signup)\")\n",
    "print()\n",
    "\n",
    "retention_results = []\n",
    "for name, period_days, min_age in periods:\n",
    "    result = calculate_retention(retention_df, period_days, min_age)\n",
    "    retention_results.append({\n",
    "        'period': name,\n",
    "        'period_days': period_days,\n",
    "        'eligible': result['eligible'],\n",
    "        'retained': result['retained'],\n",
    "        'rate': result['rate']\n",
    "    })\n",
    "    print(f\"{name}: {result['retained']}/{result['eligible']} = {result['rate']:.1f}%\")\n",
    "\n",
    "retention_curve = pd.DataFrame(retention_results)\n",
    "display(retention_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at \"days_active\" as a quality metric\n",
    "# This tells us: of all unique days since their first session, how many did they log in?\n",
    "\n",
    "print(\"=== ACTIVITY QUALITY: Days Active Distribution ===\")\n",
    "print(\"(Number of unique days they logged in)\")\n",
    "print()\n",
    "\n",
    "# Only look at companies with session data\n",
    "with_sessions = retention_df[retention_df['days_active'].notna()].copy()\n",
    "print(f\"Companies with session data: {len(with_sessions)}\")\n",
    "\n",
    "if len(with_sessions) > 0:\n",
    "    print(f\"\\nDays Active Stats:\")\n",
    "    display(with_sessions['days_active'].describe())\n",
    "    \n",
    "    # Buckets\n",
    "    print(\"\\n=== ACTIVITY BUCKETS ===\")\n",
    "    buckets = [\n",
    "        (1, 1, \"1 day only\"),\n",
    "        (2, 3, \"2-3 days\"),\n",
    "        (4, 7, \"4-7 days (about a week)\"),\n",
    "        (8, 14, \"8-14 days (1-2 weeks)\"),\n",
    "        (15, 28, \"15-28 days (2-4 weeks)\"),\n",
    "        (29, 56, \"29-56 days (1-2 months)\"),\n",
    "        (57, 999, \"57+ days (2+ months)\")\n",
    "    ]\n",
    "    \n",
    "    for min_d, max_d, label in buckets:\n",
    "        count = len(with_sessions[(with_sessions['days_active'] >= min_d) & (with_sessions['days_active'] <= max_d)])\n",
    "        pct = count / len(with_sessions) * 100\n",
    "        print(f\"  {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Build Final Analysis CSV with Retention Data\n",
    "\n",
    "Now we'll save a comprehensive `analysis_combined.csv` that includes:\n",
    "- All signup data\n",
    "- Subscription info (Brain, Connect)\n",
    "- Bot data (has_bot, has_prod_channel)\n",
    "- Payment data (actually_paid, total_paid)\n",
    "- Session/retention data (days_active, last_session, retention flags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive final analysis\n",
    "# Start with base signups (filtered to Nov 15+)\n",
    "nov15 = pd.Timestamp('2025-11-15')\n",
    "final = signups[signups['created_at'] >= nov15].copy()\n",
    "print(f\"Signups from Nov 15, 2025+: {len(final)}\")\n",
    "\n",
    "# Ensure company_id is numeric\n",
    "final['company_id'] = pd.to_numeric(final['company_id'], errors='coerce')\n",
    "\n",
    "# --- Add subscription info ---\n",
    "if subscriptions is not None:\n",
    "    subs = subscriptions.copy()\n",
    "    subs['company_id'] = pd.to_numeric(subs['company_id'], errors='coerce')\n",
    "    \n",
    "    # Companies with any subscription\n",
    "    companies_with_subs = subs['company_id'].dropna().unique()\n",
    "    final['has_subscription'] = final['company_id'].isin(companies_with_subs)\n",
    "    \n",
    "    # Active/Trialing\n",
    "    active_companies = subs[subs['status'] == 'ACTIVE']['company_id'].unique()\n",
    "    trialing_companies = subs[subs['status'] == 'TRIALING']['company_id'].unique()\n",
    "    final['has_active'] = final['company_id'].isin(active_companies)\n",
    "    final['has_trialing'] = final['company_id'].isin(trialing_companies)\n",
    "    \n",
    "    # Brain Studio\n",
    "    brain_subs = subs[subs['product_name'].str.contains('Brain', case=False, na=False)]\n",
    "    brain_companies = brain_subs['company_id'].unique()\n",
    "    final['has_brain_studio'] = final['company_id'].isin(brain_companies)\n",
    "    brain_active = brain_subs[brain_subs['status'] == 'ACTIVE']['company_id'].unique()\n",
    "    final['brain_active'] = final['company_id'].isin(brain_active)\n",
    "    \n",
    "    # Connect\n",
    "    connect_subs = subs[subs['product_name'].str.contains('Connect', case=False, na=False)]\n",
    "    connect_companies = connect_subs['company_id'].unique()\n",
    "    final['has_connect'] = final['company_id'].isin(connect_companies)\n",
    "    connect_active = connect_subs[connect_subs['status'] == 'ACTIVE']['company_id'].unique()\n",
    "    connect_trialing = connect_subs[connect_subs['status'] == 'TRIALING']['company_id'].unique()\n",
    "    final['connect_active'] = final['company_id'].isin(connect_active)\n",
    "    final['connect_trialing'] = final['company_id'].isin(connect_trialing)\n",
    "else:\n",
    "    for col in ['has_subscription', 'has_active', 'has_trialing', 'has_brain_studio', 'brain_active', \n",
    "                'has_connect', 'connect_active', 'connect_trialing']:\n",
    "        final[col] = False\n",
    "\n",
    "print(f\"Subscription columns added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add bot info ---\n",
    "if bots is not None:\n",
    "    bots_copy = bots.copy()\n",
    "    bots_copy['company_id'] = pd.to_numeric(bots_copy['company_id'], errors='coerce')\n",
    "    \n",
    "    bot_companies = bots_copy['company_id'].unique()\n",
    "    final['has_bot'] = final['company_id'].isin(bot_companies)\n",
    "    \n",
    "    # Production channel\n",
    "    prod_bots = bots_copy[bots_copy['in_production'] == 1]\n",
    "    prod_companies = prod_bots['company_id'].unique()\n",
    "    final['has_prod_channel'] = final['company_id'].isin(prod_companies)\n",
    "    \n",
    "    # Bot count\n",
    "    bot_counts = bots_copy.groupby('company_id').size().reset_index(name='bot_count')\n",
    "    final = final.merge(bot_counts, on='company_id', how='left')\n",
    "    final['bot_count'] = final['bot_count'].fillna(0).astype(int)\n",
    "else:\n",
    "    final['has_bot'] = False\n",
    "    final['has_prod_channel'] = False\n",
    "    final['bot_count'] = 0\n",
    "\n",
    "print(f\"Bot columns added\")\n",
    "\n",
    "# --- Add credit wallet info ---\n",
    "if credit_wallet is not None:\n",
    "    wallet = credit_wallet.copy()\n",
    "    wallet['company_id'] = pd.to_numeric(wallet['company_id'], errors='coerce')\n",
    "    \n",
    "    used_companies = wallet[wallet['total_used'] > 0]['company_id'].unique()\n",
    "    final['used_conversations'] = final['company_id'].isin(used_companies)\n",
    "    \n",
    "    exceeded_companies = wallet[wallet['exceeded_free_tier'] == 1]['company_id'].unique()\n",
    "    final['exceeded_free_tier'] = final['company_id'].isin(exceeded_companies)\n",
    "else:\n",
    "    final['used_conversations'] = False\n",
    "    final['exceeded_free_tier'] = False\n",
    "\n",
    "print(f\"Wallet columns added\")\n",
    "\n",
    "# --- Add payment info ---\n",
    "if stripe_invoices is not None:\n",
    "    invoices = stripe_invoices.copy()\n",
    "    invoices['company_id'] = pd.to_numeric(invoices['company_id'], errors='coerce')\n",
    "    \n",
    "    paid_invoices = invoices[invoices['amount_paid'] > 0]\n",
    "    paid_companies = paid_invoices['company_id'].unique()\n",
    "    final['actually_paid'] = final['company_id'].isin(paid_companies)\n",
    "    \n",
    "    # Total paid\n",
    "    paid_summary = paid_invoices.groupby('company_id')['amount_paid'].sum().reset_index()\n",
    "    paid_summary.columns = ['company_id', 'total_paid']\n",
    "    final = final.merge(paid_summary, on='company_id', how='left')\n",
    "    final['total_paid'] = final['total_paid'].fillna(0)\n",
    "else:\n",
    "    final['actually_paid'] = False\n",
    "    final['total_paid'] = 0\n",
    "\n",
    "print(f\"Payment columns added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add template usage info ---\n",
    "if template_usage is not None:\n",
    "    tu = template_usage.copy()\n",
    "    tu['company_id'] = pd.to_numeric(tu['company_id'], errors='coerce')\n",
    "    \n",
    "    # Aggregate by company_id (just in case there are duplicates)\n",
    "    tu_agg = tu.groupby('company_id').agg({\n",
    "        'total_events': 'sum',\n",
    "        'created_templates': 'sum',\n",
    "        'updated_templates': 'sum',\n",
    "        'deleted_templates': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    final = final.merge(tu_agg, on='company_id', how='left')\n",
    "    \n",
    "    # Fill NaNs\n",
    "    final['created_templates'] = final['created_templates'].fillna(0).astype(int)\n",
    "    final['total_template_events'] = final['total_events'].fillna(0).astype(int)\n",
    "    final['has_template_usage'] = final['created_templates'] > 0\n",
    "    \n",
    "    # Clean up duplicate total_events if any\n",
    "    if 'total_events' in final.columns:\n",
    "        final = final.drop(columns=['total_events'])\n",
    "        \n",
    "    print(f\"Template usage columns added\")\n",
    "else:\n",
    "    final['created_templates'] = 0\n",
    "    final['total_template_events'] = 0\n",
    "    final['has_template_usage'] = False\n",
    "    print(\"No template usage data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add session/retention info ---\n",
    "if 'user_sessions' in dir() and user_sessions is not None:\n",
    "    sessions = user_sessions.copy()\n",
    "    sessions['company_id'] = pd.to_numeric(sessions['company_id'], errors='coerce')\n",
    "    \n",
    "    # Merge session data\n",
    "    session_cols = ['company_id', 'first_session', 'last_session', 'days_active', 'total_sessions', 'user_count']\n",
    "    session_cols = [c for c in session_cols if c in sessions.columns]\n",
    "    \n",
    "    final = final.merge(sessions[session_cols], on='company_id', how='left')\n",
    "    \n",
    "    # Merge session duration if available\n",
    "    if 'sessions_duration' in dir() and sessions_duration is not None:\n",
    "        final = final.merge(sessions_duration, on='company_id', how='left')\n",
    "        final['total_time_minutes'] = final['total_time_minutes'].fillna(0)\n",
    "        final['avg_session_minutes'] = final['avg_session_minutes'].fillna(0)\n",
    "    \n",
    "    # Calculate retention metrics\n",
    "    final['signup_date'] = final['created_at'].dt.normalize()\n",
    "    \n",
    "    # Make sure session dates are tz-naive\n",
    "    if final['last_session'].dt.tz is not None:\n",
    "        final['last_session'] = final['last_session'].dt.tz_convert(None)\n",
    "    if final['first_session'].dt.tz is not None:\n",
    "        final['first_session'] = final['first_session'].dt.tz_convert(None)\n",
    "    \n",
    "    final['last_session_date'] = final['last_session'].dt.normalize()\n",
    "    final['first_session_date'] = final['first_session'].dt.normalize()\n",
    "    \n",
    "    # Days to last activity (key retention metric)\n",
    "    final['days_to_last_activity'] = (final['last_session_date'] - final['signup_date']).dt.days\n",
    "    \n",
    "    # Days since signup\n",
    "    today = pd.Timestamp.now().normalize()\n",
    "    final['days_since_signup'] = (today - final['signup_date']).dt.days\n",
    "    \n",
    "    # Pre-calculate retention flags for Streamlit\n",
    "    # \"Retained at Week N\" = last activity was at least N*7 days after signup\n",
    "    final['retained_day1'] = final['days_to_last_activity'] >= 1\n",
    "    final['retained_week1'] = final['days_to_last_activity'] >= 7\n",
    "    final['retained_week2'] = final['days_to_last_activity'] >= 14\n",
    "    final['retained_week3'] = final['days_to_last_activity'] >= 21\n",
    "    final['retained_week4'] = final['days_to_last_activity'] >= 28\n",
    "    final['retained_week5'] = final['days_to_last_activity'] >= 35\n",
    "    final['retained_week6'] = final['days_to_last_activity'] >= 42\n",
    "    final['retained_week7'] = final['days_to_last_activity'] >= 49\n",
    "    final['retained_week8'] = final['days_to_last_activity'] >= 56\n",
    "    \n",
    "    # Fill NaN retention flags with False (no session data = not retained)\n",
    "    for col in ['retained_day1', 'retained_week1', 'retained_week2', 'retained_week3', 'retained_week4', 'retained_week5', 'retained_week6', 'retained_week7', 'retained_week8']:\n",
    "        final[col] = final[col].fillna(False)\n",
    "    \n",
    "    # Days active as quality metric\n",
    "    final['days_active'] = final['days_active'].fillna(0).astype(int)\n",
    "    final['total_sessions'] = final['total_sessions'].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Session/retention columns added\")\n",
    "else:\n",
    "    final['days_active'] = 0\n",
    "    final['total_sessions'] = 0\n",
    "    final['retained_day1'] = False\n",
    "    final['retained_week1'] = False\n",
    "    final['retained_week2'] = False\n",
    "    final['retained_week3'] = False\n",
    "    final['retained_week4'] = False\n",
    "    final['retained_week5'] = False\n",
    "    final['retained_week6'] = False\n",
    "    final['retained_week7'] = False\n",
    "    final['retained_week8'] = False\n",
    "    print(\"No session data available\")\n",
    "\n",
    "print(f\"\\nFinal dataframe: {len(final)} rows, {len(final.columns)} columns\")\n",
    "\n",
    "    # Merge nodes usage if available\n",
    "    if 'nodes_usage' in dir() and nodes_usage is not None:\n",
    "        # Aggregate total nodes per company\n",
    "        nodes_agg = nodes_usage.groupby('company_id')['nodes_created'].sum().reset_index(name='total_nodes_created')\n",
    "        final = final.merge(nodes_agg, on='company_id', how='left')\n",
    "        final['total_nodes_created'] = final['total_nodes_created'].fillna(0).astype(int)\n",
    "    else:\n",
    "        final['total_nodes_created'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out internal Jelou accounts\n",
    "before = len(final)\n",
    "\n",
    "# Remove @jelou.ai emails\n",
    "if 'email' in final.columns:\n",
    "    final = final[~final['email'].str.contains('@jelou.ai', case=False, na=False)]\n",
    "\n",
    "# Remove jelou-related slugs\n",
    "if 'slug' in final.columns:\n",
    "    final = final[~final['slug'].str.contains('jelou', case=False, na=False)]\n",
    "\n",
    "# Remove test companies from test_companies.xlsx\n",
    "test_companies_path = 'data/test_companies.xlsx'\n",
    "if os.path.exists(test_companies_path):\n",
    "    try:\n",
    "        test_companies_df = pd.read_excel(test_companies_path)\n",
    "        first_col = test_companies_df.columns[0]\n",
    "        test_company_names = test_companies_df[first_col].dropna().str.strip().str.lower().tolist()\n",
    "        if 'company_name' in final.columns:\n",
    "            before_test = len(final)\n",
    "            final = final[~final['company_name'].str.strip().str.lower().isin(test_company_names)]\n",
    "            print(f\"Filtered out {before_test - len(final)} test companies from test_companies.xlsx\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load test_companies.xlsx: {e}\")\n",
    "\n",
    "after = len(final)\n",
    "print(f\"Removed {before - after} internal Jelou accounts\")\n",
    "print(f\"Final clean dataset: {len(final)} companies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = [2829,2831,2832,2834,2835,2839,2840,2841,2842,2844,2845,2848,2849,2851,2853,2854,2855,2856,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2882,2883,2884,2885,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2901,2902,2903,2904,2905,2906,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2952,2953,2954,2955,2956,2957,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2972,2973,2975,2976,2977,2978,2979,2980,2981,2982,2984,2985,2986,2987,2988,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3040,3041,3042,3043,3044,3045,3046,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3071,3073,3075,3076,3077,3079,3082,3085,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3155,3156,3157,3158,3159,3160,3161,3162,3163,3165,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3192,3193,3194,3195,3196,3199,3200,3201,3202,3203,3204,3205,3206,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3259,3260,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308]\n",
    "len(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and save\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(final)\n",
    "ss_total = len(final[final['plan'] == 'SELF_SERVICE'])\n",
    "\n",
    "print(f\"\\nTotal companies: {total}\")\n",
    "print(f\"  - SELF_SERVICE: {ss_total}\")\n",
    "print(f\"  - ENTERPRISE: {len(final[final['plan'] == 'ENTERPRISE'])}\")\n",
    "print(f\"  - Other: {total - ss_total - len(final[final['plan'] == 'ENTERPRISE'])}\")\n",
    "\n",
    "print(f\"\\n=== FUNNEL (SELF_SERVICE) ===\")\n",
    "ss = final[final['plan'] == 'SELF_SERVICE']\n",
    "print(f\"1. Signups: {len(ss)}\")\n",
    "print(f\"2. Created Bot: {ss['has_bot'].sum()} ({ss['has_bot'].sum()/len(ss)*100:.1f}%)\")\n",
    "print(f\"3. Production Channel: {ss['has_prod_channel'].sum()} ({ss['has_prod_channel'].sum()/len(ss)*100:.1f}%)\")\n",
    "print(f\"4. Used Conversations: {ss['used_conversations'].sum()} ({ss['used_conversations'].sum()/len(ss)*100:.1f}%)\")\n",
    "print(f\"5. Exceeded Free Tier: {ss['exceeded_free_tier'].sum()} ({ss['exceeded_free_tier'].sum()/len(ss)*100:.1f}%)\")\n",
    "print(f\"6. Actually Paid: {ss['actually_paid'].sum()} ({ss['actually_paid'].sum()/len(ss)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== RETENTION (SELF_SERVICE) ===\")\n",
    "# Only count signups old enough\n",
    "ss_7days = ss[ss['days_since_signup'] >= 7]\n",
    "ss_14days = ss[ss['days_since_signup'] >= 14]\n",
    "ss_21days = ss[ss['days_since_signup'] >= 21]\n",
    "ss_28days = ss[ss['days_since_signup'] >= 28]\n",
    "ss_35days = ss[ss['days_since_signup'] >= 35]\n",
    "ss_42days = ss[ss['days_since_signup'] >= 42]\n",
    "ss_49days = ss[ss['days_since_signup'] >= 49]\n",
    "ss_56days = ss[ss['days_since_signup'] >= 56]\n",
    "ss_63days = ss[ss['days_since_signup'] >= 63]\n",
    "\n",
    "print(f\"Day 1:  {ss_7days['retained_day1'].sum()}/{len(ss_7days)} = {ss_7days['retained_day1'].sum()/len(ss_7days)*100:.1f}%\" if len(ss_7days) > 0 else \"N/A\")\n",
    "print(f\"Week 1: {ss_14days['retained_week1'].sum()}/{len(ss_14days)} = {ss_14days['retained_week1'].sum()/len(ss_14days)*100:.1f}%\" if len(ss_14days) > 0 else \"N/A\")\n",
    "print(f\"Week 2: {ss_21days['retained_week2'].sum()}/{len(ss_21days)} = {ss_21days['retained_week2'].sum()/len(ss_21days)*100:.1f}%\" if len(ss_21days) > 0 else \"N/A\")\n",
    "print(f\"Week 3: {ss_28days['retained_week3'].sum()}/{len(ss_28days)} = {ss_28days['retained_week3'].sum()/len(ss_28days)*100:.1f}%\" if len(ss_28days) > 0 else \"N/A\")\n",
    "print(f\"Week 4: {ss_28days['retained_week4'].sum()}/{len(ss_28days)} = {ss_28days['retained_week4'].sum()/len(ss_28days)*100:.1f}%\" if len(ss_28days) > 0 else \"N/A\")\n",
    "print(f\"Week 5: {ss_35days['retained_week5'].sum()}/{len(ss_35days)} = {ss_35days['retained_week5'].sum()/len(ss_35days)*100:.1f}%\" if len(ss_35days) > 0 else \"N/A\")\n",
    "print(f\"Week 6: {ss_42days['retained_week6'].sum()}/{len(ss_42days)} = {ss_42days['retained_week6'].sum()/len(ss_42days)*100:.1f}%\" if len(ss_42days) > 0 else \"N/A\")\n",
    "print(f\"Week 7: {ss_49days['retained_week7'].sum()}/{len(ss_49days)} = {ss_49days['retained_week7'].sum()/len(ss_49days)*100:.1f}%\" if len(ss_49days) > 0 else \"N/A\")\n",
    "print(f\"Week 8: {ss_56days['retained_week8'].sum()}/{len(ss_56days)} = {ss_56days['retained_week8'].sum()/len(ss_56days)*100:.1f}%\" if len(ss_56days) > 0 else \"N/A\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n=== DAYS ACTIVE DISTRIBUTION (SELF_SERVICE) ===\")\n",
    "with_activity = ss[ss['days_active'] > 0]\n",
    "print(f\"Companies with any session: {len(with_activity)}/{len(ss)} ({len(with_activity)/len(ss)*100:.1f}%)\")\n",
    "if len(with_activity) > 0:\n",
    "    print(f\"  1 day only: {(with_activity['days_active'] == 1).sum()}\")\n",
    "    print(f\"  2-7 days: {((with_activity['days_active'] >= 2) & (with_activity['days_active'] <= 7)).sum()}\")\n",
    "    print(f\"  8-14 days: {((with_activity['days_active'] >= 8) & (with_activity['days_active'] <= 14)).sum()}\")\n",
    "    print(f\"  15-28 days: {((with_activity['days_active'] >= 15) & (with_activity['days_active'] <= 28)).sum()}\")\n",
    "    print(f\"  29+ days: {(with_activity['days_active'] >= 29) & (with_activity['days_active'] <= 35).sum()}\")\n",
    "    print(f\"  35+ days: {(with_activity['days_active'] >= 35) & (with_activity['days_active'] <= 42).sum()}\")\n",
    "    print(f\"  42+ days: {(with_activity['days_active'] >= 42) & (with_activity['days_active'] <= 49).sum()}\")\n",
    "    print(f\"  49+ days: {(with_activity['days_active'] >= 49) & (with_activity['days_active'] <= 56).sum()}\")\n",
    "    print(f\"  56+ days: {(with_activity['days_active'] >= 56) & (with_activity['days_active'] <= 63).sum()}\")\n",
    "    print(f\"  63+ days: {(with_activity['days_active'] >= 63).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Streamlit\n",
    "# Select columns to save (avoid duplicates and unnecessary columns)\n",
    "save_cols = [\n",
    "    # Core info\n",
    "    'company_id', 'company_name', 'slug', 'type', 'plan', 'email', \n",
    "    'in_production', 'state', 'environment', 'country', 'timezone',\n",
    "    'created_at', 'updated_at',\n",
    "    \n",
    "    # Subscription flags\n",
    "    'has_subscription', 'has_active', 'has_trialing',\n",
    "    'has_brain_studio', 'brain_active',\n",
    "    'has_connect', 'connect_active', 'connect_trialing', 'has_template_usage', 'created_templates', 'total_template_events',\n",
    "    \n",
    "    # Bot flags\n",
    "    'has_bot', 'has_prod_channel', 'bot_count',\n",
    "    \n",
    "    # Usage/payment flags\n",
    "    'used_conversations', 'exceeded_free_tier', 'actually_paid', 'total_paid',\n",
    "    \n",
    "    # Session/retention data\n",
    "    'first_session', 'last_session', 'days_active', 'total_sessions',\n",
    "    'days_to_last_activity', 'days_since_signup', 'total_time_minutes', 'avg_session_minutes', 'total_nodes_created',\n",
    "    'retained_day1', 'retained_week1', 'retained_week2', 'retained_week3', 'retained_week4', 'retained_week5', 'retained_week6', 'retained_week7', 'retained_week8'\n",
    "]\n",
    "\n",
    "# Only include columns that exist\n",
    "save_cols = [c for c in save_cols if c in final.columns]\n",
    "\n",
    "# Save\n",
    "final[save_cols].to_csv('data/analysis_combined.csv', index=False)\n",
    "print(f\"\\nâœ… Saved: data/analysis_combined.csv\")\n",
    "print(f\"   {len(final)} rows, {len(save_cols)} columns\")\n",
    "print(f\"\\nColumns saved:\")\n",
    "for col in save_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
